\documentclass[rnd]{mas_proposal}
% \documentclass[thesis]{mas_proposal}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\title{Object detection in adverse weather conditions using tightly-coupled data-driven multi-modal sensor fusion}
\author{Kevin Patel}
\supervisors{Prof. Dr.-Ing. Sebastian Houben\\M.Sc. Santosh Thoduka}
\date{February 2023}

% \thirdpartylogo{path/to/your/image}

\begin{document}

\maketitle

\pagestyle{plain}

\section{Introduction}

\subsection{Topic of This R\&D Project}
\begin{itemize}
% *************************
    % \item Provide reasonably detailed description of what you intent to do in your R\&D project.
    % \item You may also discuss the challenges that you have to address.
    % \item Reflect on the profile of the reader and PLEAAAASE, tell a story here and refrain from bombarding the readers with details which they may not be able to appreciate.
% **************************
    
    % \item What is Sensor Fusion?
    % \begin{itemize}
    %     \item The process of combining data from multiple sensors to provide a more accurate, reliable, and comprehensive understanding of an environment or situation.
    % \end{itemize}

    % Why do we need multi-modal sensor fusion?
        % Check where to put this question

    % \item To address this challenge, our research and development project aims to fuse different sensor modalities, including point cloud, pixel, and time series data, to improve perception and object detection accuracy in a variety of adverse weather conditions.

    % \item Fuse different sensor modalities like point cloud, pixels, and time series 
    % \item Improve perception in adverse weather conditions e.g., fog, rain, snow, overcast, sleet, night
    % \item Synchronization of multi-modal data
    % \item Process dense and sparse resolution sensors data
    % \item Make use of a data-driven approach
    % \item Geometrically alignments of different sensors
    % \item Real-time sensor fusion with low latency

    \item Imagine driving on a winding mountain road at night, with fog and rain obscuring your view, your vehicle's self-driving system struggles to detect objects ahead due to the challenging weather conditions. Suddenly, a deer jumps out in front of your car, causing the system to issue an alert and apply the brakes in time to avoid a collision.
    
    \item This scenario highlights the importance of object detection in adverse weather conditions for self-driving cars. Visual cameras, which are commonly used for object detection, may be distorted or obscured by rain, fog, snow, or low light, making it difficult to accurately detect objects on the road \cite{yurtsever2020survey} \cite{carballo2020libre} \cite{mcity2020}.
    
    \item To address these challenges, this project aims to implement a multi-modal sensor fusion system that combines cameras, radar, and lidar sensors. By fusing data from multiple sensors and leveraging advanced machine learning algorithms, the goal is to enhance object detection's range, accuracy, and reliability in adverse weather conditions.
    
    \item The focus will also be on synchronizing multi-modal data, processing dense and sparse resolution sensor data, and using a data-driven approach to optimize object detection performance.
    
    \item However, this project also faces several challenges. For example, different sensors may have different resolutions and sampling rates and may require sophisticated calibration and alignment techniques to ensure the accurate fusion of their data. Furthermore, processing large volumes of sensor data with minimal latency requires efficient and scalable algorithms and hardware architectures.
    
    \item The proposed system will be trained on a diverse dataset to ensure robustness and adaptability in different weather and lighting conditions. The system's effectiveness will be evaluated by extensive experiments and by comparing existing state-of-the-art methods.
    
    \item Despite the challenges, the project has the potential to revolutionize object detection in adverse weather conditions, with applications ranging from self-driving cars to surveillance and security systems. By fusing multiple sensor data sources and optimizing their fusion, situational awareness can be enhanced, enabling safer and more efficient operations in various domains.
    
    \item This research aims to facilitate safe and efficient self-driving in adverse weather conditions, prioritizing the safety of passengers, other drivers, and pedestrians on the road. To accomplish this, the proposed approach is to develop a sensor fusion system that operates with minimal latency, enabling data processing from multiple sensors in near real-time.
    
    % write code to put next item on new page
    \newpage

    \item Topic naming convention:
        \begin{itemize}
            \item Object detection
                \begin{itemize}
                    \item Refers to the task of detecting objects within an image or video stream.
                    \item In this project, the focus is on detecting 2D objects such as cars, trucks, pedestrians, and cyclists.
                \end{itemize}
            \item Adverse weather conditions
                \begin{itemize}
                    \item Refers to conditions such as fog, snow, rain, overcast skies, sleet, and dust.
                    \item These conditions can make object detection more challenging due to reduced visibility or other environmental factors.
                \end{itemize}
            \item Tightly-coupled
                \begin{itemize}
                    \item Refers to how different modalities of data are combined and integrated at different levels.
                    \item Rather than relying solely on early, mid, or late fusion techniques, a combination of features at different levels is employed to achieve optimal fusion results.
                \end{itemize}
            \item Data-driven
                \begin{itemize}
                    \item Refers to the use of previously collected data or publicly available datasets to improve object detection performance.
                \end{itemize}
            \item Multi-modal
                \begin{itemize}
                    \item Refers to the use of different data modalities to improve object detection performance.
                    \item Examples include sensors such as lidar, camera, IMU, GPS, infrared, and radar, with different datatypes such as point clouds, images, and time series data.
                \end{itemize}
            \item Sensor fusion
                \begin{itemize}
                    \item Refers to the process of fusing data from different sensors to get a better estimation of an environment and improve object detection performance.
                \end{itemize}
        \end{itemize}

        % \begin{itemize}
        %     \item {Object detection}
        %     \begin{itemize}
        %         \item 2D object detection - car, truck, pedestrian, cycle
        %     \end{itemize}
        
        %     \item {Adverse weather conditions}
        %     \begin{itemize}
        %         \item Fog, snow, rainy, overcast, sleet, dust
        %     \end{itemize}
        
        %     \item {Tightly-coupled}
        %     \begin{itemize}
        %         \item How different modalities are combined at what level
        %         \begin{itemize}
        %             \item Eg. sensor fusion, mid fusion/feature fusion, late fusion, ROI fusion, decision fusion
        %         \end{itemize}
        %     \end{itemize}
        
        %     \item {Data-driven}
        %     \begin{itemize}
        %         \item Using previously collected data or publicly available datasets
        %     \end{itemize}
        
        %     \item {Multi-modal}
        %     \begin{itemize}
        %         \item Using different data modalities
        %         \begin{itemize}
        %             \item Sensors: Lidar, camera, IMU, GPS, infrared, radar
        %             \item Datatypes: Point cloud, image, timer series
        %         \end{itemize}
        %     \end{itemize}
        
        %     \item {Sensor fusion}
        %     \begin{itemize}
        %         \item Fuse different sensors data to get a better estimation of an environment
        %     \end{itemize}
        % \end{itemize}


\end{itemize}

\subsection{Relevance of This R\&D Project}
\begin{itemize}
% CAGR - Compound Annual Growth Rate

    \item Weather phenomena have various negative influences on traffic and transportation. Averagely, global precipi- tation occurs 11.0\% of the time [4]. It has been proven that the risk of accident in rain conditions is 70\% higher than normal [5]. 77\% of the countries in the world re- ceive snow.
    
    \item Take the United States national statistics as an example, each year over 30,000 vehicle crashes occur on snowy or icy roads or during snowfall or sleet [6], so the threat from snow is bona fide

    \item According to Federal Highway Administration(FHA), adverse weather-related vehicle crashes cause over 5,000 fatalities and over 418,000 injuries each year in the United States. 
    \cite{federal-highway-administration-no-date}

    % Insurance Institute for Highway Safety (IIHS)
    \item The IIHS also found that in snowy weather, the fatal crash rate is 21\% higher than on clear roads, while during sleet and freezing rain, the rate is even higher at 37\%.

    \item According to the European Commission, 25\% of all road accidents in Europe happen due to adverse weather conditions, from highest to lowest: frost and ice, snow, and rain.
    \cite{cookson-2022}

    \item Autonomous vehicles: according to Marketsandmarkets, the sensor fusion market for autonomous vehicles is expected to reach \$ 22.2 billion by 2030 at a CAGR of 25.4\%  
    \cite{marketsandmarkets}

    \item According to the Federal Highway Administration (FHWA), poor visibility is a contributing factor in over 7,000 annual crashes in the United States.
    
    \item According to the National Highway Traffic Safety Administration (NHTSA), poor visibility was a contributing factor in over 4,000 fatal crashes in the United States in 2018.
    
    \item According to the IIHS, in foggy weather, fatal crashes happen at a rate that is 6 times higher than in clear weather.
    
    \item According to European Transport Safety Council (ETSC), over 12,000 people die on European roads each year in weather-related accidents, from highest to lowest: frost and ice, snow, and rain.
    
    \item Not only this, there are other sectors, for example, healthcare for wearable sensors, precision agriculture, and environmental monitoring, that have also seen the fruitful impact of multi-modal sensor fusion.

    \item \textbf{Healthcare sector}: wearable sensors, estimated that the global wearable device market is expected to reach over \$ 54 billion in revenue by 2027, growing at a CAGR of over 13\%
    
    \item \textbf{Precision agriculture and  environmental monitoring}: for better crop health and analyze deforestation, \$45 billion by 2026, growing at a CAGR of over 20\% 
    
    \item \textbf{Aerospace and defense}: including aircraft navigation and control, missile guidance, and military logistics. Expected to reach \$4.71 billion by 2025, at a CAGR of 8.2\%    

    \item \textbf{Industrial automation}:  increase the efficiency and productivity of manufacturing processes, as well as reduce the risk of errors and accidents
    

    
% *************************    
    % \item Who will benefit from the results of this R\&D project?
    % \item What are the benefits? Quantify the benefits with concrete numbers.
% *************************
 \end{itemize}

\section{Related Work}

\subsection{Survey of Related Work}
\begin{itemize}
    \item Bijelic et al. employed a deep learning-based transfer learning approach to address unseen adverse weather conditions
    \cite{bijelic2020seeing}
    \begin{itemize}
        \item 5 sensor modalities C-R-L-FIR-NIR
    \end{itemize}
    
    \item K-radar: 
    \cite{Paek2022Jun}
    \begin{itemize}
        \item Released 4D radar dataset
        \item Showed baseline network only, and mAP still 41.1\%
        \item But not compared with other multi-modal architectures and does not use advanced NN techniques 
    \end{itemize}
        
    
    \item C-R Fusion:
    \cite{Nobis2020May}
    \begin{itemize}
        \item Model inspired by C-L fusion
        \item Shows the importance of radar in object detection
    \end{itemize}    
    
    
    % ***********************
    % \item What have other people done to solve the problem?
    % \item You should reference and briefly discuss at least the ``top twelve'' related works
    % ***********************
\end{itemize}

\subsection{Limitation and Deficits in the State of the Art}
\begin{itemize}

    \item Most existing works fuse RGB images from visual cameras with 3D LiDAR point clouds    
    \cite{feng2020deep}
    
    \item There is no general guideline for network architecture design, and the below questions are still open\cite{Zhou2022May}: 
        \begin{itemize}
            \item “what to fuse” - lidar, radar, color camera, thermal camera, event camera, ultrasonic 
            \item “how to fuse” - addition or mean, concatenate, ensemble, mixture of experts
            \item “when to fuse” - early, mid, late, combination of all
        \end{itemize}

    \item Previous studies lack comparison with alternative models or datasets
    \item showing only results for their own baseline models and custom datasets
    
    \item None of the multi-modal sensor fusion algorithms handle temporal information
    \cite{bijelic2020seeing}
    
    \item Not much work available utilizing 4D imaging radar sensor 
    \cite{Zhou2022May}    


    % ***********************
    % \item List the deficits that you have discovered in the related work and explain them such that a person who is not deep into the technical details can still understand them.
    % For each deficit, provide at least two references
    % \item You should reference and briefly discuss at least the ``top twelve'' related works
    % ***********************
\end{itemize}

\section{Problem Statement}
\begin{itemize}
    
    \item Which of the deficits are you going to solve?
    
    \item What is your intended approach?
    \item A thorough analysis and practical implementation of state-of-the-art methods for object detection using multiple modalities including but not limited to camera, lidar, and radar
        
    \item Determining an appropriate fusion strategy to exploit the complementary characteristics of various sensors
    \begin{itemize}
        \item How to fuse camera + 4D radar data
    \end{itemize}
    
    \item Fusion of spatial and temporal information from multi-modal sensors

    \item If required, use CARLA or other simulators to validate the performance of a model 
    
    \item Conduct experiments and compare outcomes with various models and adverse weather conditions datasets
    \begin{itemize}
        \item Datasets: K-radar\cite{Paek2022Jun}, DENSE\cite{bijelic2020seeing}, aiMotive\cite{Matuszka2022Nov}
    \end{itemize}    


    \item How will you compare you approach with existing approaches?
\end{itemize}

\section{Project Plan}

\subsection{Work Packages}
\emph{Planning is the replacement of randomness by error.} (Einstein). Very much like you would never start a longer journey without a detailed travel plan, you should not start a project without a carefully though out work plan. A work package is a logical decomposition of a larger piece of work into smaller parts following a ``divide and conquer" strategy. It is very specific to the problem that you are going to address. Refrain from a rather generic decomposition. If your work plan looks similar to those of your school mates, which may address completely different problems then you have not thought carefully enough about how you approach the problem. It is ok to have two generic work packages \emph{Literature Study} and \emph{Project Report}. Discuss your work packages in the ASW seminar.

The bare minimum will include the following packages:
\begin{enumerate}
    \item[WP1] Literature Study
    \item[WP2] ...
    \item[WP3] ...
    \item  ...
    \item[WPy] Evaluation of approach and comparison with similar approaches
    \item[WPz] Project Report
\end{enumerate}

\subsection{Milestones}
Milestones mark the completion of a certain activity or at least a major achievement in an activity. Milestones are also decision points, where you reflect on what you have achieved and what options you have for continuing your work in case you have not achieved what was planned. Above all, milestones have to be measurable. As above, if your milestones are the same as those of your school mates, then you may not have thought carefully enough about how your project shall progress.
\begin{enumerate}
    \item[M1] Literature review completed and best practice identified
    \item[M2] ...
    \item[M3] ...
    \item[M4] Report submission
\end{enumerate}

\subsection{Project Schedule}
Include a Gantt chart here. It doesn't have to be detailed, but it should include the milestones you mentioned above.
Make sure to include the writing of your report throughout the whole project, not just at the end.

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{images/rnd_deliverable_timeline}
    \caption{My figure caption}
    \label{fig:myfigure}
\end{figure}

\subsection{Deliverables}

\subsubsection*{Minimum Viable}
\begin{itemize}
    \item Project results required to get a satisfying or sufficient grade.
\end{itemize}

\subsubsection*{Expected}
\begin{itemize}
    \item Project results required to get a good grade.
\end{itemize}

\subsubsection*{Desired}
\begin{itemize}
    \item Project results required to get an excellent grade.
\end{itemize}

Please note that the final grade will not only depend on the results obtained in your work, but also on how you present the results.

\nocite{*}

\bibliographystyle{unsrt} % Use the plainnat bibliography style
\bibliography{bibliography.bib} % Use the bibliography.bib file as the source of references

\end{document}